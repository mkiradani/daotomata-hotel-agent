# AI Evaluation Suite for Hotel Agent

Sistema completo de evaluaci√≥n con AI para probar conversaciones del agente hotelero, validar respuestas y generar an√°lisis detallados.

## üéØ Caracter√≠sticas

### ‚úÖ Funcionalidades Implementadas

- **Simulador de Conversaciones**: Simula interacciones reales entre clientes y el agente hotelero
- **Evaluaci√≥n con AI**: Usa GPT-4 para evaluar la calidad, precisi√≥n y utilidad de las respuestas
- **Pruebas de Herramientas**: Verifica el funcionamiento de todas las herramientas del agente
- **Sistema de Logging**: Registro completo y detallado de todas las interacciones y evaluaciones
- **Evaluaci√≥n Secundaria**: AI adicional que analiza los resultados y patrones del sistema
- **M√∫ltiples Hoteles**: Soporte para probar diferentes configuraciones hoteleras
- **Exportaci√≥n de Datos**: Resultados en JSON, CSV y reportes detallados

### üè® Hoteles de Prueba Configurados

1. **Hotel Madrid Palace** (Madrid) - Hotel de lujo con spa, gym, piscina
2. **Barcelona Beach Resort** (Barcelona) - Resort costero con acceso a playa
3. **Sevilla Heritage Hotel** (Sevilla) - Hotel boutique hist√≥rico

### üé≠ Escenarios de Prueba

1. **basic_info_inquiry** - Consulta de informaci√≥n b√°sica del hotel
2. **availability_check** - Verificaci√≥n de disponibilidad de habitaciones  
3. **weather_and_activities** - Consulta del clima y actividades locales
4. **service_request** - Solicitud de servicios del hotel
5. **complex_multi_tool** - Escenario complejo que requiere m√∫ltiples herramientas

### üõ†Ô∏è Herramientas Probadas

- `get_hotel_info` - Informaci√≥n del hotel
- `check_room_availability` - Disponibilidad de habitaciones
- `get_hotel_activities` - Actividades disponibles
- `get_hotel_facilities` - Instalaciones del hotel
- `get_local_weather` - Informaci√≥n meteorol√≥gica
- `request_hotel_service` - Solicitud de servicios
- Herramientas PMS (Cloudbeds integration)

## üöÄ Uso R√°pido

### Instalaci√≥n

```bash
# Instalar dependencias
pip install -r requirements.txt

# Configurar API key de OpenAI
export OPENAI_API_KEY="tu_api_key_aqu√≠"
```

### Ejecutar Evaluaci√≥n Completa

```bash
# Evaluaci√≥n completa (herramientas + conversaciones + an√°lisis secundario)
python -m tests.ai_evaluation.main --run-all

# Solo pruebas de herramientas
python -m tests.ai_evaluation.main --tools-only

# Solo conversaciones y evaluaci√≥n
python -m tests.ai_evaluation.main --conversations-only

# Escenarios espec√≠ficos
python -m tests.ai_evaluation.main --conversations-only --scenarios basic_info_inquiry availability_check

# Hoteles espec√≠ficos
python -m tests.ai_evaluation.main --run-all --hotels hotel_madrid_luxury
```

## üìä Resultados y An√°lisis

### Archivos Generados

```
logs/ai_evaluation/
‚îú‚îÄ‚îÄ conversations_YYYYMMDD_HHMMSS.jsonl    # Conversaciones detalladas
‚îú‚îÄ‚îÄ evaluations_YYYYMMDD_HHMMSS.jsonl      # Evaluaciones AI
‚îú‚îÄ‚îÄ results_YYYYMMDD_HHMMSS.csv            # Resultados en CSV
‚îú‚îÄ‚îÄ summary_YYYYMMDD_HHMMSS.json           # Resumen de sesi√≥n
‚îú‚îÄ‚îÄ system_YYYYMMDD_HHMMSS.log             # Logs del sistema
‚îú‚îÄ‚îÄ tool_test_results_YYYYMMDD_HHMMSS.json # Resultados de herramientas
‚îî‚îÄ‚îÄ secondary_analysis_report_YYYYMMDD_HHMMSS.json # An√°lisis secundario
```

### M√©tricas de Evaluaci√≥n

- **Overall Score** (0.0-1.0): Puntuaci√≥n general de la conversaci√≥n
- **Accuracy**: Precisi√≥n de la informaci√≥n proporcionada
- **Helpfulness**: Utilidad de las respuestas para el cliente
- **Tool Usage**: Uso apropiado de las herramientas disponibles
- **Conversation Flow**: Fluidez y coherencia de la conversaci√≥n
- **Politeness**: Profesionalidad y cortes√≠a en el trato

### An√°lisis Secundario

El evaluador secundario proporciona:

- **Pattern Analysis**: Patrones de √©xito y fallo
- **System Performance Score**: Puntuaci√≥n general del sistema
- **Key Findings**: Hallazgos m√°s importantes
- **Strategic Recommendations**: Recomendaciones de mejora
- **Quality Assessment**: Evaluaci√≥n de la calidad del sistema de evaluaci√≥n

## üîß Configuraci√≥n Avanzada

### A√±adir Nuevos Hoteles

Edita `tests/ai_evaluation/config.py`:

```python
TEST_HOTELS.append(HotelConfig(
    id=\"nuevo_hotel_id\",
    name=\"Nuevo Hotel\",
    city=\"Ciudad\",
    coordinates=(lat, lon),
    contact_email=\"contacto@hotel.com\",
    contact_phone=\"+34 XXX XXX XXX\",
    description=\"Descripci√≥n del hotel\",
    facilities=[\"spa\", \"gym\", \"pool\"],
    activities=[...]
))
```

### A√±adir Nuevos Escenarios

```python
TEST_SCENARIOS.append(ConversationScenario(
    scenario_id=\"nuevo_escenario\",
    hotel_id=\"hotel_id\",
    title=\"T√≠tulo del Escenario\",
    description=\"Descripci√≥n detallada\",
    initial_message=\"Mensaje inicial del cliente\",
    expected_tools=[\"tool1\", \"tool2\"],
    success_criteria=[\"Criterio 1\", \"Criterio 2\"]
))
```

### Criterios de Evaluaci√≥n

Modifica `EVALUATION_CONFIG` en `config.py`:

```python
EVALUATION_CONFIG = EvaluationCriteria(
    accuracy_weight=0.3,      # Peso de precisi√≥n
    helpfulness_weight=0.2,   # Peso de utilidad
    tool_usage_weight=0.2,    # Peso de uso de herramientas
    conversation_flow_weight=0.15,  # Peso de fluidez
    politeness_weight=0.15,   # Peso de cortes√≠a
    minimum_passing_score=0.75  # Puntuaci√≥n m√≠nima para aprobar
)
```

## üìà Interpretaci√≥n de Resultados

### Puntuaciones de √âxito

- **0.90-1.00**: Excelente - El agente maneja la conversaci√≥n perfectamente
- **0.75-0.89**: Bueno - Conversaci√≥n exitosa con mejoras menores
- **0.60-0.74**: Aceptable - Funcional pero necesita mejoras
- **0.00-0.59**: Necesita Mejora - Problemas significativos identificados

### Indicadores Clave

- **Pass Rate**: % de conversaciones que superan el umbral m√≠nimo
- **Tool Coverage**: % de herramientas esperadas que se usaron correctamente
- **Average Response Time**: Tiempo promedio de respuesta del agente
- **Error Rate**: % de conversaciones que fallaron por errores t√©cnicos

### Recomendaciones T√≠picas

1. **Improve tool utilization** - Usar m√°s herramientas relevantes
2. **Enhance response accuracy** - Mejorar la precisi√≥n de la informaci√≥n
3. **Better conversation flow** - Mantener mejor contexto conversacional
4. **Optimize response time** - Reducir tiempos de respuesta
5. **Professional tone consistency** - Mantener tono profesional

## üîç Debugging y Soluci√≥n de Problemas

### Logs Detallados

```bash
# Ejecutar con salida verbose
python -m tests.ai_evaluation.main --run-all --verbose
```

### Problemas Comunes

1. **OpenAI API Key**: Aseg√∫rate de tener configurada la variable de entorno
2. **PMS Integration**: Algunas herramientas pueden fallar si no hay integraci√≥n PMS
3. **Rate Limiting**: El sistema incluye delays autom√°ticos para evitar l√≠mites de API
4. **Memory Usage**: Las evaluaciones completas pueden usar bastante memoria

### Configuraci√≥n de API

```bash
# Usar API key espec√≠fica
python -m tests.ai_evaluation.main --run-all --openai-api-key \"tu_key\"

# Variables de entorno adicionales
export OPENAI_ORG_ID=\"org_id_opcional\"
```

## ü§ù Contribuci√≥n

### A√±adir Nuevos Tipos de Evaluaci√≥n

1. Extiende `EvaluationCriteria` para nuevas m√©tricas
2. Modifica `AIEvaluator._evaluate_single_response` para nuevos criterios
3. Actualiza prompts de evaluaci√≥n en `evaluator.py`

### Mejorar An√°lisis Secundario

1. Extiende `PatternAnalysis` para nuevos patrones
2. A√±ade m√©todos de an√°lisis en `AdvancedSecondaryEvaluator`
3. Actualiza prompts de an√°lisis para nuevos insights

## üìö Arquitectura del Sistema

```
tests/ai_evaluation/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ config.py           # Configuraci√≥n de hoteles y escenarios
‚îú‚îÄ‚îÄ simulator.py        # Simulador de conversaciones
‚îú‚îÄ‚îÄ evaluator.py        # Evaluador AI principal
‚îú‚îÄ‚îÄ secondary_evaluator.py  # Evaluador AI secundario
‚îú‚îÄ‚îÄ tool_tests.py       # Pruebas de herramientas
‚îú‚îÄ‚îÄ logger.py           # Sistema de logging
‚îú‚îÄ‚îÄ main.py            # Script principal
‚îî‚îÄ‚îÄ README.md          # Esta documentaci√≥n
```

### Flujo de Ejecuci√≥n

1. **Setup**: Configuraci√≥n de hoteles y agentes de prueba
2. **Tool Testing**: Verificaci√≥n de funcionamiento de herramientas
3. **Conversation Simulation**: Simulaci√≥n de conversaciones reales
4. **AI Evaluation**: Evaluaci√≥n de calidad con GPT-4
5. **Secondary Analysis**: An√°lisis de patrones y meta-evaluaci√≥n
6. **Reporting**: Generaci√≥n de reportes y exportaci√≥n de datos

## üéØ Casos de Uso

### Desarrollo

- Validar nuevas funcionalidades antes del deploy
- Identificar regresiones en el comportamiento del agente
- Optimizar prompts y l√≥gica de herramientas

### QA y Testing

- Pruebas automatizadas de regresi√≥n
- Validaci√≥n de calidad en diferentes hoteles
- Benchmarking de rendimiento

### An√°lisis de Producto

- Identificar patrones de uso de herramientas
- Optimizar flujos de conversaci√≥n m√°s comunes
- An√°lisis de satisfacci√≥n de usuarios simulados

### Investigaci√≥n y Mejora

- Experimentos A/B con diferentes configuraciones
- An√°lisis de efectividad de diferentes estrategias
- Identificaci√≥n de oportunidades de mejora

---

**Nota**: Este sistema est√° dise√±ado para ser extensible y configurable. Puedes adaptarlo f√°cilmente a tus necesidades espec√≠ficas modificando los archivos de configuraci√≥n y a√±adiendo nuevos tipos de evaluaci√≥n.